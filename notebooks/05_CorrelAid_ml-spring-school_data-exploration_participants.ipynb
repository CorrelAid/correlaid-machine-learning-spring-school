{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51f0c45b-e0ff-4569-8eea-fa723eb564c4",
   "metadata": {},
   "source": [
    "# CorrelAid machine-learning spring school \n",
    "## Section 5: Data Exploration \n",
    "\n",
    "Author: Sebastian Zezulka \n",
    "\n",
    "Session will take place on **Tuesday, 29.03.2022, 18h CEST**.\n",
    "\n",
    "**to be distributed on Thursday, 24.03.2022**\n",
    "\n",
    "During the next weeks, you will analyse the \"Forest Cover Type Dataset\" from [UCI Machine Learning](https://archive.ics.uci.edu/ml/datasets/covertype). In this section, you will learn how to set up a (primary) data exploration and to prepare your data for further (machine learning) analyses.\n",
    "Doing this, you will also take a close look at the practical problem and the data. For data extrapolation, you will have to estimate descriptive statistics and build some visualisations of the data. Further, you will have to write a function to get a balanced dataset and build a training-test-split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaa7fb6-9d11-4266-83d7-27ad421b9d14",
   "metadata": {},
   "source": [
    "### 0. Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8c48a8-723e-404c-a9bc-681e5ea6fae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f286c9b7-46ae-43c7-82ff-840a1e60bc30",
   "metadata": {},
   "source": [
    "### 1. Load Data\n",
    "**Task:** Download the CSV data from the given URL/repo and inspect a few rows from it. What size has the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173886f7-a518-4745-8d6b-95505b1a66cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())\n",
    "  \n",
    "# to change the working directory use\n",
    "# os.chdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f1d875-ea1d-4735-8f56-95f1c5689bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in csv data\n",
    "\n",
    "# INSERT YOUR CODE HERE\n",
    "forest_data = []\n",
    "\n",
    "# Inspect first few rows of the data using the .head() command\n",
    "\n",
    "# INSERT YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf329cfb-e00b-420e-941b-7930a648b32e",
   "metadata": {},
   "source": [
    "## 2. Exploratory Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6fc148-df06-42f2-a98b-f0c125d698fc",
   "metadata": {},
   "source": [
    "**Task:** Let's take a first look at the data we are going to analyse in the next weeks. Please make some notes for the workshop. We will predict the type of cover of a forest based on several features (variables) available. What, then, is the dependent variable in our data given the task? What was the original purpose to collect the data at hand? Do you have an intuition which features might be especially important and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383d518c-00f5-4b23-be1b-2d9dc73cbd39",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e69bfc-646d-4967-92c0-3369049190e1",
   "metadata": {},
   "source": [
    "**Task:** Does the dataset contain missing values? Check how many categories are in 'Type of Cover'. How many observations do we have per category? Plot the number of observations per category. What do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187c3ae7-9ccb-435b-ba9f-095d1fa6763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values (e.g. using the .info() command)\n",
    "\n",
    "# INSERT YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afec6f85-b5d4-4ece-8304-d3de8b311cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of observations per cover type category \n",
    "\n",
    "# INSERT YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1f5992-7d19-482d-adc2-a79e281cd631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar plot of observations per cover type category \n",
    "\n",
    "# INSERT YOUR CODE HERE\n",
    "list_soil_types = []\n",
    "num_obs = []\n",
    "\n",
    "plt.bar(list_soil_types, num_obs)\n",
    "plt.xlabel('Category Type of Cover')\n",
    "plt.ylabel('# of observations')\n",
    "plt.title('Number of observations per Type of Cover category, original data')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc86e449-08d8-4418-be77-5faf94f5a1d5",
   "metadata": {},
   "source": [
    "**Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfca78d-a9ef-456d-8548-4c07311213a9",
   "metadata": {},
   "source": [
    "**Task:** In the task above, we have seen that our data is unbalanced in the target variable `Type of Cover`. That is, there are different number of observations per category. This can be problematic for some machine learning algorithms, especially in classification, because much more information on classes 1 and 2 is available in training. This also can have implications for fairness concerns. In principle, we have three easy ways to deal with this problem: learn on the unbalanced data anyways; use *undersampling* by reducing the number of observations to the class with lowest observations; use *oversampling* by multiplying observations from categories with smaller number of observations or weight the loss function accordingly. You can get a more detailed overview [here](https://medium.com/strands-tech-corner/unbalanced-datasets-what-to-do-144e0552d9cd)\n",
    "\n",
    "Here, we will apply undersampling only. Your task is to write a function that produces a balanced dataset by randomly sampling from the respective classes. For this, use the provided random number generator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfca7f1-cfab-4578-a898-925fe33c891b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersampling(data, y_label: str, seed: int):\n",
    "    '''\n",
    "    Function that generates a balanced dataset from an unbalanced one\n",
    "    input: \n",
    "        data : (N, D)-dataset\n",
    "        y_label (string) : name of target variable\n",
    "        seed (int) : seed for random number generator \n",
    "    \n",
    "    output:\n",
    "        data_balanced : dataset with balanced classes\n",
    "    '''\n",
    "    rng = np.random.default_rng(seed)\n",
    "    \n",
    "    # INSERT YOUR CODE HERE\n",
    "\n",
    "    \n",
    "    raise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26a74aa-28d2-4d69-89db-a0dcd0b0e1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_data_balanced = undersampling(forest_data, 'Cover_Type', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1ccf87-61ce-4481-afa7-941ed39a3624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the undersampling function by plotting the number of observations per category\n",
    "\n",
    "# INSERT YOUR CODE HERE\n",
    "list_soil_types = []\n",
    "num_obs = []\n",
    "\n",
    "plt.bar(list_soil_types, num_obs)\n",
    "plt.xlabel('Category type of Cover')\n",
    "plt.ylabel('# of observations')\n",
    "plt.title('Number of observations per type of Cover category, balanced data')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7394c2-6f48-4a5a-b570-080acd934463",
   "metadata": {},
   "source": [
    "**Task:** Two of our variables (`Wilderness_Area` and `Soil_Type`) are OneHot-encoded (also called dummy variables). That is, for every category we have one variable which indicates membership with a '1' and '0' elsewhere. For the analysis, change the OneHote encoded variables of \"Type Wilderness\" and \"Type Soil\" into a categorical variable each and drop the dummies. That is, if a variable has `Soil_Type2`==1, it now should have \"2\" in an overall variable `Soil_Type`. How many categories has each variable and what do they encode, respectivley? Is every category present in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fefb07-d777-41a2-a8dc-83417a17a733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all variable names from dataset \n",
    "display(forest_data_balanced.columns)\n",
    "\n",
    "# transform dummy variables into categorical ones and drop dummies\n",
    "\n",
    "# INSERT YOUR CODE HERE\n",
    "\n",
    "\n",
    "# transform strings to numbers by, first, deleting text (use str.replace()) and, second, transforming variables to numeric\n",
    "\n",
    "# INSERT YOUR CODE HERE\n",
    "\n",
    "# drop dummy variables\n",
    "\n",
    "# INSERT YOUR CODE HERE\n",
    "\n",
    "# check result\n",
    "display(forest_data_balanced.head())\n",
    "display(forest_data_balanced.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d33a1ef-6eee-4118-a639-8152e3743d62",
   "metadata": {},
   "source": [
    "**Task:** Reorder the features in the dataset such that our feature variable `Cover_Type` is the last feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73db1690-71bb-4239-a6c6-2f05b569aa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12abce7-228c-4002-a101-635da432eb67",
   "metadata": {},
   "source": [
    "**Task:** Now, let us take a closer look at the data and some descriptive statistics. Please answer the following questions: What is the `Cover Type` with the highest mean distance to the next fire point? What is the standard deviation of hillshades at noon? What is measured by the features `Elevation`, `Aspect`, and `Slope`? What is their respective scale?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a743703c-8e3f-4727-b108-def495fb0f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Descriptive Statistics (e.g. use the .describe() command)\n",
    "\n",
    "# INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c3e568-2ddb-496d-abfc-6f75d2f7e21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordered distance to fire points by 'Cover Type' categories\n",
    "\n",
    "# INSERT YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237f1e9b-558a-4262-8def-2cd0a0685a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard deviation of the feature 'Hillshade at Noon'\n",
    "\n",
    "# INSERT YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817c310f-b3fb-4ff0-89fa-131616ea5c9c",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bacf3ec-58e4-4eeb-a10f-6375f0592bb7",
   "metadata": {},
   "source": [
    "**Task:** In this last part of Section 2, we will look at some visualisations of the features. First, we will consider correlations between variables. \n",
    "Create a visual representation of the correlation matrix of the features. Do you find any patterns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59052a82-77ec-4ad4-9fd2-8a1e001c9769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matrix of Features\n",
    "\n",
    "# INSERT YOUR CODE HERE\n",
    "labels = []\n",
    "cor = []\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "im = plt.imshow(cor)\n",
    "\n",
    "plt.xticks(ticks=x_, labels=labels, rotation=90)\n",
    "plt.yticks(ticks=x_, labels=labels, rotation=0)\n",
    "fig.colorbar(im, ax=ax)\n",
    "plt.title('Correlation Matrix of numerical features')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f5d82b-4bf9-40aa-891a-655447427bb9",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc75e5c-8e80-43b7-a513-3028ef1c9b6d",
   "metadata": {},
   "source": [
    "**Task:** Second, we will look at scatter plots between pairs of features. You can use the 'PairGrid'-function from [seaborn](https://seaborn.pydata.org/generated/seaborn.PairGrid.html) for this. On the diagonal, we plot the histograms of the respective feature. As before, can you make out any patterns in the data? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23530d5a-c2fd-44cf-92bb-d8e8c195b159",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.PairGrid(data = forest_data_balanced.iloc[:,:10], corner=True)\n",
    "\n",
    "# INSERT YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac521c3-02b4-460f-b858-006785ded1ad",
   "metadata": {},
   "source": [
    "**Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de4dab1-6227-403f-9a45-a3ff83030082",
   "metadata": {},
   "source": [
    "### 3. PCA and data-compression\n",
    "This is a small exkurs! We won't discuss it (in detail) in the workshop, so feel free to skipp this part.\n",
    "\n",
    "PCA - principal component analysis - is a simple (linear algebra) tool of unsupervised learning. In general, there are three motivations for using dimensionality reduction techniques: saving memory, finding \"structure\" in the data and finding differences between a known number of classes in the data. PCA can also be used as preprocessing step in a machine learning pipline. The core idea of PCA is to find a lower dimensional representation of the data such that the mean squared reconstruction error is minimised. One can show that this is achieved for projecting the (centered) data on the first m eigenvectors of the covariance (or correlation) matrix. Note that scaling of data changes the results. Also, it is hard (to impossible) to interpret results when features have not the same unit! \n",
    "\n",
    "You can find an intuitive explanation of PCA [here](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues). The same author, Dmitry Kobak, has an hour long introduction to PCA online [here](https://www.youtube.com/watch?v=xBf_LZ5ZgY4). Overall, his series of ten talks is a fantastic introduction to machine learning!\n",
    "\n",
    "**Task:** Write a function that performs PCA on input data. For this, estimate the covariance matrix of the data and estimate its eigenvalues. Sort them in decreasing order and sort the corresponding eigenvectors accordingly. The eigenvalues dividided by the total variance give the fraction of variance \"explained\" by one principle component. Multiplying the eigenvectors with the data results in the principal components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c831166c-45a2-4004-9598-46b9dbeae375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA(data):\n",
    "    '''\n",
    "    Function that performs PCA on the input data\n",
    "    \n",
    "    input: (N, D)-shaped data\n",
    "    output:\n",
    "        fraction_variance_explained: (D,)-shaped array with the fraction of variance explained by the individual PCs\n",
    "        principal_components: (D, D)-shaped array containing the principal components as columns\n",
    "    '''\n",
    "    # INSERT YOUR CODE HERE\n",
    "    \n",
    "    # covariate matrix of transposed data\n",
    "    \n",
    "    # eigenvalues and -vectors of cov-matrix\n",
    "    \n",
    "    # sort eigenvalues and eigenvectors \n",
    "\n",
    "    # total variance is trace of C\n",
    "\n",
    "    # Explained Variance by PC i \n",
    "\n",
    "    # principal components\n",
    "  \n",
    "    raise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18eea190-643d-44b6-a209-726a5297236d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only numerical features for PCA\n",
    "pca_forest_data_balanced = forest_data_balanced.iloc[:,:10]\n",
    "\n",
    "frac_var_exp, PC_forest_data = PCA(pca_forest_data_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760834fa-19a4-4f32-ac3f-942600d6e849",
   "metadata": {},
   "source": [
    "**Task:** We first want to know how much variance the first principle components explain. For this, plot the variance explained of the `n`-th PC on the y-axis against the number of principal components on the x-axis. Second, plot the cumulative variance explained for the `n` PCs with highest variance explained (y-axis vs. the number of principal components (x-axis). \n",
    "From the latter plot you should be able to see how much PCs you need to keep to explain at least `x`% of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d87fbcf-e8ca-4e9a-9165-064291882fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,7))\n",
    "\n",
    "# INSERT YOUR CODE HERE\n",
    "fig = []\n",
    "fig2 = []\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(fig)\n",
    "plt.title('variance explained by the n\\'th PC')\n",
    "plt.ylabel('share of variance explained in %')\n",
    "plt.xlabel('n\\'th PC')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(fig2)\n",
    "plt.title('cummulative variance explained by PCs')\n",
    "plt.ylabel('cum. share of variance explained in %')\n",
    "plt.xlabel(r'n\\'th PC')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a38af0-e59e-4b99-aa2a-6f140cef56c4",
   "metadata": {},
   "source": [
    "**Task:** We now can also plot the first three PC scores, that is the data multiplied with the first `n` PCs against each other. Further, by colouring the datapoints by `Cover_Type` categories, we can see whether the first few principle components can separete the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b037ac3a-a059-4977-b2d1-f5da32523718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA scores of forest_data: multiply the data with the three eigenvectors that belong to the two largest eigenvalues\n",
    "\n",
    "# INSERT YOUR CODE HERE\n",
    "\n",
    "\n",
    "plt.scatter(pca_scores.iloc[:,0], pca_scores.iloc[:,1], c=forest_data_balanced['Cover_Type'])\n",
    "plt.xlabel('PC1 score')\n",
    "plt.ylabel('PC2 score')\n",
    "plt.title('Biplot of principle component scores')\n",
    "plt.show();\n",
    "plt.scatter(pca_scores.iloc[:,1], pca_scores.iloc[:,2], c=forest_data_balanced['Cover_Type'])\n",
    "plt.xlabel('PC2 score')\n",
    "plt.ylabel('PC3 score')\n",
    "plt.title('Biplot of principle component scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d78dbd-604c-48c5-8216-f95bb420de7d",
   "metadata": {},
   "source": [
    "**Task:** Lastly, implement the PCA algorithm using the sklearn package and compare the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aba935d-18fc-4897-acad-b7aff7f1e165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c926144-696a-41c5-8c60-262985a57483",
   "metadata": {},
   "source": [
    "### 4. Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c699b611-b1d5-477c-9a33-a3f472c1a383",
   "metadata": {},
   "source": [
    "In this final section, we prepare the data for further (machine learning) analyses. For this, we split the data in a training and a test set and standardise it.\n",
    "\n",
    "A core feature of machine learning is the usage of *strictily sparated* trainings and test data to check the generlisibility of a model. The background to this is so called \"bias-variance trade-off\". Intuitivley, we can have so many parameters that the model perfectly learns the data. That implies a zero loss on this data - after all, we have fitted a model perfectly on this data. The problem is that this approach will (in almost all cases) give really bad results when using this model on any new data. We say our model \"overfitted\". To control the generlisation performance, we train the model only on a subset of the available data (say, 80%) and test on the rest (20% then). Ultimatley, we are interested in a model that performs well on the test set, not the training. You will hear more about this in the next weeks. Only the test set result is a reasonable approximation of the performance of your model! \n",
    "\n",
    "When you have panel data and you assume they are indepent and identically distributed (iid), it is important to shuffle your data before you make the split to ensure that the data collection process does not \"encode\" information in your datasets. Note the difference to time-series data, where iid assumption is violated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08eea9a-ea6b-4591-b4a0-c0b751951517",
   "metadata": {},
   "source": [
    "**Task:** Before, we have collapsed the OneHot encoded variables into one categorical. For the analysis, we have to re-transform the variables `Wilderness_Area_Type` and `Soil_Type ` into a OneHot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7870241-e46a-4fba-89d6-dab87433c7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHotEncoding for `Wilderness Area'\n",
    "\n",
    "# INSERT YOUR CODE HERE\n",
    "\n",
    "print('Categories for Wilderness Area: ' + str(ohe.categories_))\n",
    "\n",
    "# OneHotEncoding for `Soil Type'\n",
    "\n",
    "# INSERT YOUR CODE HERE\n",
    "\n",
    "print('Categories for Soil Type: ' + str(ohe2.categories_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf22e54-6cf3-4c4a-8637-43cf05fba525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make pd.DataFrame with correct indices\n",
    "\n",
    "# INSERT YOUR CODE HERE\n",
    "\n",
    "\n",
    "# merge OneHot encodings with the original dataset\n",
    "\n",
    "# INSERT YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744dbdf9-7e4f-407c-a471-52ae891b30b1",
   "metadata": {},
   "source": [
    "**Task:** Now separate the dependent variable `y` from the rest of the data, `X`, and drop it. Then use the `train_test_split` function to create a test and trainings set, the test set should have size 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb0a4b1-7f81-43d8-afb4-2e27c857cc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate the target feature in an array `y` and drop it from the rest of the data `X`\n",
    "\n",
    "# INSERT YOUR CODE HERE\n",
    "\n",
    "\n",
    "# test and training set split\n",
    "\n",
    "# INSERT YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55de7a46-0f93-4f5a-842b-b1da1e47f63f",
   "metadata": {},
   "source": [
    "In this last step, we standardise the continuous variables. That means one both centers the variable (i.e. subtracts the means) and divides by the standard deviation of the training data so that the variable has a standard deviations of 1. Importantly, the test data is standardised by using the statistics of the training data. If we standardise the whole dataset before splitting, we risk \"information leakage\" into the test set. Categorical (dummy) variables are left out and are later re-added. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08aec8f-a586-4964-a5e1-9226cff0ab27",
   "metadata": {},
   "source": [
    "**Task:** Split of the categorical and numerical parts of both trainings and test data, use the instantiated function `StandardScale` to standardise both trainings and test data. Then, rejoin the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774c3709-975a-43b8-acc4-5cda1cb2955f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split of numerical variables for standardisation\n",
    "\n",
    "# INSERT YOUR CODE HERE\n",
    "\n",
    "\n",
    "# standardise numerical data\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "\n",
    "# INSERT YOUR CODE HERE\n",
    "\n",
    "\n",
    "# use column names and indices from above to transform scaled results in DataFrames\n",
    "\n",
    "# INSERT YOUR CODE HERE\n",
    "\n",
    "\n",
    "# rejoin the numerical and categorical data in two DataFrames: one for the scaled trainings and one for the scaled test data\n",
    "\n",
    "# INSERT YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38454c3a-63f6-456e-a0f2-e33161aa8cf3",
   "metadata": {},
   "source": [
    "**Task:** Lastly, save the two dataframes as csv files for future analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb83d1c-7452-43e4-bafd-5281d8ecd855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data as csv files\n",
    "\n",
    "# INSERT YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b350c07b-c1d3-4f08-b1cb-5fe834327c6a",
   "metadata": {},
   "source": [
    "### Sources:\n",
    "The PCA part is modelled after the course Machine Learning I by Dmitry Kobak, Uni Tübingen. \n",
    "\n",
    "Inspiration and idea for sns.PairGrid comes from Ceren Iyim (2020). [Predicting Forest Cover Types with the Machine Learning Workflow](https://towardsdatascience.com/predicting-forest-cover-types-with-the-machine-learning-workflow-1f6f049bf4df). Medium.\n",
    "\n",
    "Thanks to Philipp Holzträger and Alexander Braun for helpful comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b453456-c90a-49c2-81ad-d6cb6b2d00d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
