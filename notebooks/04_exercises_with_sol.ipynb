{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f0cabe8",
   "metadata": {},
   "source": [
    "## Exercises for workshop 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0b70f2",
   "metadata": {},
   "source": [
    "Even though we do not have a strong focus on mathematics in this workshop series, we encourage you to get familiar with some mathematical topics since machine learning in general is highly depending on them. If you want to you can use the following materials provided by the Standford university to review or refresh your mathematical knowledge:\n",
    "- Linear algebra http://cs229.stanford.edu/section/cs229-linalg.pdf\n",
    "- Probability theory http://cs229.stanford.edu/summer2020/cs229-prob.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "915efd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math as m\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dceb7e",
   "metadata": {},
   "source": [
    "We have 10 samples from the cities Berlin, Munich, Stuttgart, Nuremberg, Hamburg, Hannover, Augsburg, Halle, Fürth and Ingolstadt.\n",
    "\n",
    "To make things easier we include the bias term in our feature array by adding a 1 to every sample of longitude and latitude data. We get the following feature, label and weight arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b94dd56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array([[1, 52.5167, 13.3833], [1, 48.1372, 11.5755], [1, 48.7761, 9.1775], [1, 49.4539, 11.0775], \n",
    "                     [1, 53.55, 10], [1, 52.3744, 9.7386], [1, 48.3717, 10.8983], [1, 51.4828, 11.9697], \n",
    "                     [1, 49.4783, 10.9903], [1, 48.7636, 11.4261]]) # [biasTerm, lat, lng]\n",
    "\n",
    "labels = np.array([0, 1, 0, 1, 0, 0, 1, 0, 1, 1]) # is in Bavaria\n",
    "\n",
    "\n",
    "weights = np.array([1,.5,.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12872c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0 /(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9230f1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(features, weights):\n",
    "    return sigmoid(np.dot(features, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "470b940a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(features, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86672674",
   "metadata": {},
   "source": [
    "### Task 1:\n",
    "You are given the mentioned parametric function. Compute the binary cross entropy loss for every sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd7196a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BCE(features, weights, labels):\n",
    "    # TODO\n",
    "    predictions = predict(features, weights)\n",
    "    loss1 = -labels*np.log(predictions)\n",
    "    loss2 = (1-labels)*np.log(1-predictions)\n",
    "    BCE = loss1 - loss2\n",
    "    return BCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56e53751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.39642118e+01, 3.97459843e-14, 2.99775453e+01, 2.64233080e-14,\n",
       "       3.27855569e+01, 3.20546693e+01, 4.95159469e-14, 3.27114489e+01,\n",
       "       2.73114864e-14, 3.13082893e-14])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BCE(features, weights, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "239a339d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EmpiricalLoss(features, weights, labels):\n",
    "    N = len(features)\n",
    "    BiCrEn = BCE(features, weights, labels)\n",
    "    loss = BiCrEn.sum() / N\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d40c295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.149343221904143"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EmpiricalLoss(features, weights, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fff2fc",
   "metadata": {},
   "source": [
    "### Task 2:\n",
    "Compute the gradient of the empirical loss (also called cost function) with the binary\n",
    "cross entropy loss function with regards to the weight vector\n",
    "w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0c7a731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gradient(features,weights,labels):\n",
    "    # TODO\n",
    "    gradient = np.dot(features.T, predict(features, weights) - labels)\n",
    "    return gradient / len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aae3c525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.5    , 25.87   ,  5.42691])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Gradient(features, weights, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d5c4fa",
   "metadata": {},
   "source": [
    "### Task 3:\n",
    "Implement the gradient descent method and execute it with a learning rate of 0.001 and about 3000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90adeb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradientDescent(eta, features, labels, weights, iterations):\n",
    "    # TODO\n",
    "    for i in range(iterations):\n",
    "        weights = weights - eta*Gradient(features, weights, labels)\n",
    "        if (i % 1000 == 0):\n",
    "            print(\"iteration: \", str(i), \" loss: \", str(EmpiricalLoss(features, weights, labels)))\n",
    "    return weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5db12ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  0  loss:  15.449124167438217\n",
      "iteration:  1000  loss:  0.6036136116977191\n",
      "iteration:  2000  loss:  0.5981876720530922\n",
      "iteration:  3000  loss:  0.5950049075701982\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.02499631, -0.15097511,  0.58963827])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_weights = GradientDescent(0.001, features, labels, weights, 3001)\n",
    "updated_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557dd301",
   "metadata": {},
   "source": [
    "### Task 4:\n",
    "In the following we want to test how good our small modell is performing. For that we need a classifier which determines acording to our probalities given by our modell if a city is located in bavaria. \n",
    "After implementing the classifier compute the accuracy of your training data and the given test data. You can also take a look at the actual probabillities. Did the empirical loss improve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9de22c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data: Würzburg, Rostock, Trier, Rosenheim, Regensburg\n",
    "\n",
    "test_features = np.array([[1, 49.7944, 9.9294], [1, 54.0833, 12.1333], [1, 49.7567, 6.6414], [1, 47.8561, 12.1289], \n",
    "                    [1, 49.0167, 12.0833]]) # [biasTerm, lat, lng]\n",
    "\n",
    "test_labels = np.array([1, 0, 0, 1, 1]) # is in Bavaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78389861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(features, weights):\n",
    "    # TODO\n",
    "    prob = predict(features, weights)\n",
    "    return np.greater_equal(prob, .5)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64a64e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilites:  [0.72861119 0.64171951 0.28341687 0.52258621 0.23806128 0.24232346\n",
      " 0.53696454 0.57692632 0.50882561 0.59872907]\n",
      "Classified:  [1 1 0 1 0 0 1 1 1 1]\n",
      "Actual labels:  [0 1 0 1 0 0 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Probabilites: \", predict(features, updated_weights))\n",
    "\n",
    "print(\"Classified: \", classify(features, updated_weights))\n",
    "print(\"Actual labels: \", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "673f82e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = sum(labels==classify(features, updated_weights))/len(features)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64aa1341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.149343221904143\n",
      "0.5950049075701982\n"
     ]
    }
   ],
   "source": [
    "print(EmpiricalLoss(features, weights, labels))\n",
    "print(EmpiricalLoss(features, updated_weights, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f08fce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilites of test data:  [0.34570684 0.50350551 0.07102808 0.72143283 0.67906339]\n",
      "Classified test data:  [0 1 0 1 1]\n",
      "Actual labels of test data:  [1 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Probabilites of test data: \", predict(test_features, updated_weights))\n",
    "\n",
    "print(\"Classified test data: \", classify(test_features, updated_weights))\n",
    "print(\"Actual labels of test data: \", test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5fd53756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = sum(test_labels==classify(test_features, updated_weights))/len(test_features)\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
