{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51f0c45b-e0ff-4569-8eea-fa723eb564c4",
   "metadata": {},
   "source": [
    "# CorrelAid machine-learning spring school \n",
    "## Section 5: Data Exploration \n",
    "\n",
    "Author: Sebastian Zezulka \n",
    "\n",
    "Session will take place on **Tuesday, 29.03.2022, 18h CEST**.\n",
    "\n",
    "**to be distributed on Thursday, 24.03.2022**\n",
    "\n",
    "During the next weeks, you will analyse the \"Forest Cover Type Dataset\" from [UCI Machine Learning](https://archive.ics.uci.edu/ml/datasets/covertype). In this section, you will learn how to set up a (primary) data exploration and to prepare your data for further (machine learning) analyses.\n",
    "Doing this, you will also take a close look at the practical problem and the data. For data extrapolation, you will have to estimate descriptive statistics and build some visualisations of the data. Further, you will have to write a function to get a balanced dataset and build a training-test-split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaa7fb6-9d11-4266-83d7-27ad421b9d14",
   "metadata": {},
   "source": [
    "### 0. Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e8c48a8-723e-404c-a9bc-681e5ea6fae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f286c9b7-46ae-43c7-82ff-840a1e60bc30",
   "metadata": {},
   "source": [
    "### 1. Load Data\n",
    "**Task:** Download the CSV data from the given URL/repo and inspect a few rows from it. What size has the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "173886f7-a518-4745-8d6b-95505b1a66cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sebastian\\Documents\\10_CorrelAid\\02_Projekte\\02_ml_winter_school\\notebooks\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "  \n",
    "# to change the working directory use\n",
    "#os.chdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69f1d875-ea1d-4735-8f56-95f1c5689bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <th>...</th>\n",
       "      <th>Soil_Type32</th>\n",
       "      <th>Soil_Type33</th>\n",
       "      <th>Soil_Type34</th>\n",
       "      <th>Soil_Type35</th>\n",
       "      <th>Soil_Type36</th>\n",
       "      <th>Soil_Type37</th>\n",
       "      <th>Soil_Type38</th>\n",
       "      <th>Soil_Type39</th>\n",
       "      <th>Soil_Type40</th>\n",
       "      <th>Cover_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2596</td>\n",
       "      <td>51</td>\n",
       "      <td>3</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "      <td>510</td>\n",
       "      <td>221</td>\n",
       "      <td>232</td>\n",
       "      <td>148</td>\n",
       "      <td>6279</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2590</td>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>212</td>\n",
       "      <td>-6</td>\n",
       "      <td>390</td>\n",
       "      <td>220</td>\n",
       "      <td>235</td>\n",
       "      <td>151</td>\n",
       "      <td>6225</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2804</td>\n",
       "      <td>139</td>\n",
       "      <td>9</td>\n",
       "      <td>268</td>\n",
       "      <td>65</td>\n",
       "      <td>3180</td>\n",
       "      <td>234</td>\n",
       "      <td>238</td>\n",
       "      <td>135</td>\n",
       "      <td>6121</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2785</td>\n",
       "      <td>155</td>\n",
       "      <td>18</td>\n",
       "      <td>242</td>\n",
       "      <td>118</td>\n",
       "      <td>3090</td>\n",
       "      <td>238</td>\n",
       "      <td>238</td>\n",
       "      <td>122</td>\n",
       "      <td>6211</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2595</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>153</td>\n",
       "      <td>-1</td>\n",
       "      <td>391</td>\n",
       "      <td>220</td>\n",
       "      <td>234</td>\n",
       "      <td>150</td>\n",
       "      <td>6172</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Elevation  Aspect  Slope  Horizontal_Distance_To_Hydrology  \\\n",
       "0       2596      51      3                               258   \n",
       "1       2590      56      2                               212   \n",
       "2       2804     139      9                               268   \n",
       "3       2785     155     18                               242   \n",
       "4       2595      45      2                               153   \n",
       "\n",
       "   Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n",
       "0                               0                              510   \n",
       "1                              -6                              390   \n",
       "2                              65                             3180   \n",
       "3                             118                             3090   \n",
       "4                              -1                              391   \n",
       "\n",
       "   Hillshade_9am  Hillshade_Noon  Hillshade_3pm  \\\n",
       "0            221             232            148   \n",
       "1            220             235            151   \n",
       "2            234             238            135   \n",
       "3            238             238            122   \n",
       "4            220             234            150   \n",
       "\n",
       "   Horizontal_Distance_To_Fire_Points  ...  Soil_Type32  Soil_Type33  \\\n",
       "0                                6279  ...            0            0   \n",
       "1                                6225  ...            0            0   \n",
       "2                                6121  ...            0            0   \n",
       "3                                6211  ...            0            0   \n",
       "4                                6172  ...            0            0   \n",
       "\n",
       "   Soil_Type34  Soil_Type35  Soil_Type36  Soil_Type37  Soil_Type38  \\\n",
       "0            0            0            0            0            0   \n",
       "1            0            0            0            0            0   \n",
       "2            0            0            0            0            0   \n",
       "3            0            0            0            0            0   \n",
       "4            0            0            0            0            0   \n",
       "\n",
       "   Soil_Type39  Soil_Type40  Cover_Type  \n",
       "0            0            0           5  \n",
       "1            0            0           5  \n",
       "2            0            0           2  \n",
       "3            0            0           2  \n",
       "4            0            0           5  \n",
       "\n",
       "[5 rows x 55 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in csv data\n",
    "\n",
    "# INSERT YOUR CODE HERE\n",
    "path_name = 'data/covtype.csv'\n",
    "forest_data = pd.read_csv(path_name)\n",
    "\n",
    "# Inspect first few rows of the data\n",
    "\n",
    "# INSERT YOUR CODE HERE\n",
    "forest_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf329cfb-e00b-420e-941b-7930a648b32e",
   "metadata": {},
   "source": [
    "## 2. Exploratory Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6fc148-df06-42f2-a98b-f0c125d698fc",
   "metadata": {},
   "source": [
    "**Task:** Let's take a first look at the data we are going to analyse in the next weeks. Please make some notes for the workshop. We will predict the type of cover of a forest based on several features (variables) available. What, then, is the dependent variable in our data given the task? What was the original purpose to collect the data at hand? Do you have an intuition which features might be especially important and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383d518c-00f5-4b23-be1b-2d9dc73cbd39",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Answer:** \n",
    "\n",
    "**### delete before distribution ###**\n",
    "'Cover_Type' is the dependent variable in our setting. 7 Categories are recorded: \n",
    "1 -- Spruce/Fir\n",
    "2 -- Lodgepole Pine\n",
    "3 -- Ponderosa Pine\n",
    "4 -- Cottonwood/Willow\n",
    "5 -- Aspen\n",
    "6 -- Douglas-fir\n",
    "7 -- Krummholz.\n",
    "Data was collected to predict information on neighbouring lands of natural resource managers responsible for developing ecosystem management strategies. The study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e69bfc-646d-4967-92c0-3369049190e1",
   "metadata": {},
   "source": [
    "**Task:** Does the dataset contain missing values? Check how many categories are in 'Type of Cover'. How many observations do we have per category? Plot the number of observations per category. What do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "187c3ae7-9ccb-435b-ba9f-095d1fa6763e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'info'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\SEBAST~1\\AppData\\Local\\Temp/ipykernel_4128/2769109137.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# INSERT YOUR CODE HERE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mforest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Cover_Type'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5485\u001b[0m         ):\n\u001b[0;32m   5486\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5487\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5489\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'info'"
     ]
    }
   ],
   "source": [
    "# check for missing values\n",
    "\n",
    "# INSERT YOUR CODE HERE\n",
    "forest_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afec6f85-b5d4-4ece-8304-d3de8b311cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    283301\n",
       "1    211840\n",
       "3     35754\n",
       "7     20510\n",
       "6     17367\n",
       "5      9493\n",
       "4      2747\n",
       "Name: Cover_Type, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of observations per cover type category \n",
    "\n",
    "# INSERT YOUR CODE HERE\n",
    "forest_data.groupby('Cover_Type').count()\n",
    "\n",
    "# easier even is\n",
    "forest_data['Cover_Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1f5992-7d19-482d-adc2-a79e281cd631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar plot of observations per cover type category \n",
    "\n",
    "# INSERT YOUR CODE HERE\n",
    "list_soil_types = list(range(1,8))\n",
    "num_obs = forest_data.groupby('Cover_Type').count().iloc[:,0]\n",
    "\n",
    "plt.bar(list_soil_types, num_obs)\n",
    "plt.xlabel('Category type of Cover')\n",
    "plt.ylabel('# of observations')\n",
    "plt.title('Number of observations per type of Cover category, original data')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc86e449-08d8-4418-be77-5faf94f5a1d5",
   "metadata": {},
   "source": [
    "**Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfca78d-a9ef-456d-8548-4c07311213a9",
   "metadata": {},
   "source": [
    "**Task:** In the task above, we have seen that our data is unbalanced in the target variable `Type of Cover`. That is, there are different number of observations per category. This can be problematic for some machine learning algorithms, especially in classification, because much more information on classes 1 and 2 is available in training. This also can have implications for fairness concerns. In principle, we have three easy ways to deal with this problem: learn on the unbalanced data anyways; use *undersampling* by reducing the number of observations to the class with lowest observations; use *oversampling* by multiplying observations from categories with smaller number of observations or weight the loss function accordingly. You can get a more detailed overview [here](https://medium.com/strands-tech-corner/unbalanced-datasets-what-to-do-144e0552d9cd)\n",
    "\n",
    "Here, we will apply undersampling only. Your task is to write a function that produces a balanced dataset by randomly sampling from the respective classes. For this, use the provided random number generator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfca7f1-cfab-4578-a898-925fe33c891b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersampling(data, y_label: str, seed: int):\n",
    "    '''\n",
    "    Function that generates a balanced dataset from an unbalanced one\n",
    "    input: \n",
    "        data : (N, D)-dataset\n",
    "        y_label (string) : name of target variable\n",
    "        seed (int) : seed for random number generator \n",
    "    \n",
    "    output:\n",
    "        data_balanced : dataset with balanced classes\n",
    "    '''\n",
    "    rng = np.random.default_rng(seed)\n",
    "    \n",
    "    # INSERT YOUR CODE HERE\n",
    "    classes_y = sorted(data[y_label].unique())\n",
    "    counts_y = data.groupby(y_label).count().iloc[:,0]\n",
    "    \n",
    "    data_balanced = []\n",
    "    for i in classes_y:\n",
    "        idx = rng.integers(low=0, high=counts_y[i], size=min(counts_y))\n",
    "        data_i = pd.DataFrame(data[data[y_label] == i]).reset_index()\n",
    "        data_i = data_i.loc[idx]\n",
    "        data_balanced.append(data_i)\n",
    "        \n",
    "    data_balanced = pd.concat(data_balanced)\n",
    "    \n",
    "    return data_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26a74aa-28d2-4d69-89db-a0dcd0b0e1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_data_balanced = undersampling(forest_data, 'Cover_Type', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1ccf87-61ce-4481-afa7-941ed39a3624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the undersampling function\n",
    "list_soil_types = sorted(forest_data_balanced['Cover_Type'].unique())\n",
    "num_obs = forest_data_balanced.groupby('Cover_Type').count().iloc[:,0]\n",
    "\n",
    "plt.bar(list_soil_types, num_obs)\n",
    "plt.xlabel('Category type of Cover')\n",
    "plt.ylabel('# of observations')\n",
    "plt.title('Number of observations per type of Cover category, balanced data')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7394c2-6f48-4a5a-b570-080acd934463",
   "metadata": {},
   "source": [
    "**Task:** Two of our variables (`Wilderness_Area` and `Soil_Type`) are OneHot-encoded (also called dummy variables). That is, for every category we have one variable which indicates membership with a '1' and is '0' elsewhere. For the analysis, transform the variables of \"Type Wilderness\" and \"Type Soil\" into two categorical variables and drop the dummies. How many categories has each variable and what do they encode, respectivley? Is every category present in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fefb07-d777-41a2-a8dc-83417a17a733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all variable names from dataset \n",
    "display(forest_data_balanced.columns)\n",
    "\n",
    "# transform dummy variables into categorical ones and drop dummies\n",
    "\n",
    "# INSERT YOUR CODE HERE\n",
    "forest_data_balanced['Wilderness_Area_Type'] = (forest_data_balanced.iloc[:,11:15] == 1).idxmax(1)\n",
    "forest_data_balanced['Soil_Type'] = (forest_data_balanced.iloc[:,15:55] == 1).idxmax(1)\n",
    "\n",
    "# transform strings to numbers by, first, deleting text (use str.replace()) and, second, transforming variables to numeric\n",
    "forest_data_balanced['Wilderness_Area_Type'] = forest_data_balanced['Wilderness_Area_Type'].str.replace('Wilderness_Area', '').astype('int64') # 'category'\n",
    "forest_data_balanced['Soil_Type'] = forest_data_balanced['Soil_Type'].str.replace('Soil_Type', '').astype('int64') # 'category'\n",
    "\n",
    "# drop dummy variables\n",
    "forest_data_balanced.drop(['Wilderness_Area1',\n",
    "       'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4',\n",
    "       'Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5',\n",
    "       'Soil_Type6', 'Soil_Type7', 'Soil_Type8', 'Soil_Type9', 'Soil_Type10',\n",
    "       'Soil_Type11', 'Soil_Type12', 'Soil_Type13', 'Soil_Type14',\n",
    "       'Soil_Type15', 'Soil_Type16', 'Soil_Type17', 'Soil_Type18',\n",
    "       'Soil_Type19', 'Soil_Type20', 'Soil_Type21', 'Soil_Type22',\n",
    "       'Soil_Type23', 'Soil_Type24', 'Soil_Type25', 'Soil_Type26',\n",
    "       'Soil_Type27', 'Soil_Type28', 'Soil_Type29', 'Soil_Type30',\n",
    "       'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34',\n",
    "       'Soil_Type35', 'Soil_Type36', 'Soil_Type37', 'Soil_Type38',\n",
    "       'Soil_Type39', 'Soil_Type40'], axis=1, inplace=True)\n",
    "\n",
    "# check result\n",
    "display(forest_data_balanced.head())\n",
    "display(forest_data_balanced.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d33a1ef-6eee-4118-a639-8152e3743d62",
   "metadata": {},
   "source": [
    "**Task:** Reorder the features in the dataset such that our feature variable `Cover_Type` is the last feature and also stored as Categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73db1690-71bb-4239-a6c6-2f05b569aa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT YOUR CODE HERE\n",
    "\n",
    "forest_data_balanced['Cover_Type'] = forest_data_balanced['Cover_Type'].astype('int64') # 'category'\n",
    "\n",
    "cols_at_end = ['Cover_Type']\n",
    "forest_data_balanced = forest_data_balanced[[c for c in forest_data_balanced.columns.values if c not in cols_at_end] \n",
    "        + [c for c in cols_at_end if c in forest_data_balanced.columns.values]]\n",
    "\n",
    "display(forest_data_balanced.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12abce7-228c-4002-a101-635da432eb67",
   "metadata": {},
   "source": [
    "**Task:** Now, let us take a closer look at the data and some descriptive statistics. Please answer the following questions: What is the `Cover Type` with the highest mean distance to the next fire point? What is the standard deviation of hillshades at noon? What is measured by the features `Elevation`, `Aspect`, and `Slope`? What is their respective scale?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a743703c-8e3f-4727-b108-def495fb0f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Descriptive Statistics\n",
    "forest_data_describe = forest_data_balanced.describe()\n",
    "display(forest_data_describe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c3e568-2ddb-496d-abfc-6f75d2f7e21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordered distance to fire points by 'Cover Type' categories\n",
    "\n",
    "# INSERT YOUR CODE HERE\n",
    "forest_data_balanced.groupby('Cover_Type').mean()['Horizontal_Distance_To_Fire_Points'].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237f1e9b-558a-4262-8def-2cd0a0685a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard deviation of the feature 'Hillshade at Noon'\n",
    "\n",
    "# INSERT YOUR CODE HERE\n",
    "forest_data_balanced['Hillshade_Noon'].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817c310f-b3fb-4ff0-89fa-131616ea5c9c",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "- Cover Type 2 has largest mean of horizontal distance to next fire point: 2168.15 meters \n",
    "- std of Hillshade_Noon: 19.768\n",
    "- Elevation: meters \n",
    "- Aspect:  degrees azimuth (https://en.wikipedia.org/wiki/Azimuth)\n",
    "- Slope: degrees "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bacf3ec-58e4-4eeb-a10f-6375f0592bb7",
   "metadata": {},
   "source": [
    "**Task:** In this last part of Section 2, we will look at some visualisations of the features. First, we will consider correlations between variables. \n",
    "Create a visual representation of the correlation matrix of the features. Do you find any patterns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59052a82-77ec-4ad4-9fd2-8a1e001c9769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matrix of Features\n",
    "\n",
    "# INSERT YOUR CODE HERE\n",
    "labels = forest_data_balanced.columns\n",
    "x_ = range(len(labels))\n",
    "\n",
    "cor = forest_data_balanced[labels].corr()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "im = plt.imshow(cor)\n",
    "plt.xticks(ticks=x_, labels=labels, rotation=90)\n",
    "plt.yticks(ticks=x_, labels=labels, rotation=0)\n",
    "fig.colorbar(im, ax=ax)\n",
    "plt.title('Correlation Matrix of numerical features')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f5d82b-4bf9-40aa-891a-655447427bb9",
   "metadata": {},
   "source": [
    "**Answer:** Correlations between Hillshades at different points in time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc75e5c-8e80-43b7-a513-3028ef1c9b6d",
   "metadata": {},
   "source": [
    "**Task:** Second, we will look at scatter plots between pairs of features. You can use the 'PairGrid'-function from [seaborn](https://seaborn.pydata.org/generated/seaborn.PairGrid.html) for this. On the diagonal, we plot the histograms of the respective feature. As before, can you make out any patterns in the data? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23530d5a-c2fd-44cf-92bb-d8e8c195b159",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.PairGrid(data = forest_data_balanced.iloc[:,:10], corner=True)\n",
    "\n",
    "# INSERT YOUR CODE HERE\n",
    "\n",
    "# Diagonal is a histogram\n",
    "grid.map_diag(plt.hist, color = 'green', edgecolor = 'white')\n",
    "\n",
    "# Bottom is scatter plot\n",
    "grid.map_lower(sns.scatterplot, color = 'green', alpha = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac521c3-02b4-460f-b858-006785ded1ad",
   "metadata": {},
   "source": [
    "**Answer:** Something something one what one sees in these plots..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de4dab1-6227-403f-9a45-a3ff83030082",
   "metadata": {},
   "source": [
    "### 3. PCA and data-compression\n",
    "This is a small exkurs! We won't discuss it (in detail) in the workshop, so feel free to skipp this part.\n",
    "\n",
    "PCA - principal component analysis - is a simple (linear algebra) tool of unsupervised learning. In general, there are three motivations for using dimensionality reduction techniques: saving memory, finding \"structure\" in the data and finding differences between a known number of classes in the data. PCA can also be used as preprocessing step in a machine learning pipline. The core idea of PCA is to find a lower dimensional representation of the data such that the mean squared reconstruction error is minimised. One can show that this is achieved for projecting the (centered) data on the first m eigenvectors of the covariance (or correlation) matrix. Note that scaling of data changes the results. Also, it is hard (to impossible) to interpret results when features have not the same unit! \n",
    "\n",
    "You can find an intuitive explanation of PCA [here](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues). The same author, Dmitry Kobak, has an hour long introduction to PCA online [here](https://www.youtube.com/watch?v=xBf_LZ5ZgY4). Overall, his series of ten talks is a fantastic introduction to machine learning!\n",
    "\n",
    "**Task:** Write a function that performs PCA on input data. For this, estimate the covariance matrix of the data and estimate its eigenvalues. Sort them in decreasing order and sort the corresponding eigenvectors accordingly. The eigenvalues dividided by the total variance give the fraction of variance \"explained\" by one principle component. Multiplying the eigenvectors with the data results in the principal components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c831166c-45a2-4004-9598-46b9dbeae375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA(data):\n",
    "    '''\n",
    "    Function that performs PCA on the input data\n",
    "    \n",
    "    input: (N, D)-shaped data\n",
    "    output:\n",
    "        fraction_variance_explained: (D,)-shaped array with the fraction of variance explained by the individual PCs\n",
    "        principal_components: (D, D)-shaped array containing the principal components as columns\n",
    "    '''\n",
    "    # INSERT YOUR CODE HERE\n",
    "    \n",
    "    # covariate matrix of transposed data\n",
    "    C = np.cov(data.T)\n",
    "    # eigenvalues and -vectors of cov-matrix\n",
    "    eigen_values, eigen_vectors = np.linalg.eigh(C)\n",
    "    \n",
    "    # sort eigenvalues and eigenvectors \n",
    "    idx = eigen_values.argsort()[::-1]\n",
    "    eigen_values = eigen_values[idx]\n",
    "    eigen_vectors = eigen_vectors[:,idx]\n",
    "\n",
    "    # total variance is trace of C\n",
    "    tot_var = np.trace(C)\n",
    "    # Explained Variance by PC i \n",
    "    fraction_variance_explained = np.divide(eigen_values, tot_var)\n",
    "    \n",
    "    # principal components\n",
    "    principal_components = eigen_vectors\n",
    "        \n",
    "    return fraction_variance_explained, principal_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18eea190-643d-44b6-a209-726a5297236d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only numerical features for PCA\n",
    "pca_forest_data_balanced = forest_data_balanced.iloc[:,:10]\n",
    "\n",
    "frac_var_exp, PC_forest_data = PCA(pca_forest_data_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760834fa-19a4-4f32-ac3f-942600d6e849",
   "metadata": {},
   "source": [
    "**Task:** We first want to know how much variance the first principle components explain. For this, plot the variance explained of the `n`-th PC on the y-axis against the number of principal components on the x-axis. Second, plot the cumulative variance explained for the `n` PCs with highest variance explained (y-axis vs. the number of principal components (x-axis). \n",
    "From the latter plot you should be able to see how much PCs you need to keep to explain at least `x`% of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d87fbcf-e8ca-4e9a-9165-064291882fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,7))\n",
    "\n",
    "plt.subplot(121)\n",
    "# INSERT YOUR CODE HERE\n",
    "plt.plot( range(len(frac_var_exp)), frac_var_exp, lw=2)\n",
    "\n",
    "plt.title('variance explained by the n\\'th PC')\n",
    "plt.ylabel('share of variance explained in %')\n",
    "plt.xlabel('n\\'th PC')\n",
    "\n",
    "plt.subplot(122)\n",
    "# INSERT YOUR CODE HERE\n",
    "plt.plot(range(len(frac_var_exp)), np.cumsum(frac_var_exp), lw=2)\n",
    "\n",
    "plt.title('cummulative variance explained by PCs')\n",
    "plt.ylabel('cum. share of variance explained in %')\n",
    "plt.xlabel(r'n\\'th PC')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a38af0-e59e-4b99-aa2a-6f140cef56c4",
   "metadata": {},
   "source": [
    "**Task:** We now can also plot the first three PC scores, that is the data multiplied with the first `n` PCs against each other. Further, by colouring the datapoints by `Cover_Type` categories, we can see whether the first few principle components can separete the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b037ac3a-a059-4977-b2d1-f5da32523718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA scores of forest_data: multiply the data with the three eigenvectors that belong to the two largest eigenvalues\n",
    "\n",
    "# INSERT YOUR CODE HERE\n",
    "pca_scores =  pca_forest_data_balanced @ PC_forest_data[:,:3]\n",
    "\n",
    "plt.scatter(pca_scores.iloc[:,0], pca_scores.iloc[:,1], c=forest_data_balanced['Cover_Type'])\n",
    "plt.xlabel('PC1 score')\n",
    "plt.ylabel('PC2 score')\n",
    "plt.title('Biplot of principle component scores')\n",
    "plt.show();\n",
    "plt.scatter(pca_scores.iloc[:,1], pca_scores.iloc[:,2], c=forest_data_balanced['Cover_Type'])\n",
    "plt.xlabel('PC2 score')\n",
    "plt.ylabel('PC3 score')\n",
    "plt.title('Biplot of principle component scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d78dbd-604c-48c5-8216-f95bb420de7d",
   "metadata": {},
   "source": [
    "**Task:** Lastly, implement the PCA algorithm using the sklearn package and compare the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438b7fb9-ea4e-4821-93a3-476bc01752a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aba935d-18fc-4897-acad-b7aff7f1e165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT YOUR CODE HERE\n",
    "pca_sklearn = PCA(svd_solver='full')\n",
    "pca_sklearn.fit(pca_forest_data_balanced)\n",
    "    \n",
    "pca_sklearn_frac_var_explained = pca_sklearn.explained_variance_ratio_\n",
    "pca_sklearn_principal_components = pca_sklearn.components_.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c926144-696a-41c5-8c60-262985a57483",
   "metadata": {},
   "source": [
    "### 4. Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c699b611-b1d5-477c-9a33-a3f472c1a383",
   "metadata": {},
   "source": [
    "In this final section, we prepare the data for further (machine learning) analyses. For this, we split the data in a training and a test set and standardise it.\n",
    "\n",
    "A core feature of machine learning is the usage of *strictily sparated* trainings and test data to check the generlisibility of a model. The background to this is so called \"bias-variance trade-off\". Intuitivley, we can have so many parameters that the model perfectly learns the data. That implies a zero loss on this data - after all, we have fitted a model perfectly on this data. The problem is that this approach will (in almost all cases) give really bad results when using this model on any new data. We say our model \"overfitted\". To control the generlisation performance, we train the model only on a subset of the available data (say, 80%) and test on the rest (20% then). Ultimatley, we are interested in a model that performs well on the test set, not the training. You will hear more about this in the next weeks. Only the test set result is a reasonable approximation of the performance of your model! \n",
    "\n",
    "When you have panel data and you assume they are indepent and identically distributed (iid), it is important to shuffle your data before you make the split to ensure that the data collection process does not \"encode\" information in your datasets. Note the difference to time-series data, where iid assumption is violated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08eea9a-ea6b-4591-b4a0-c0b751951517",
   "metadata": {},
   "source": [
    "**Task:** Before, we have collapsed the OneHot encoded variables into one categorical. For the analysis, we have to re-transform the variables `Wilderness_Area_Type` and `Soil_Type ` into a OneHot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7870241-e46a-4fba-89d6-dab87433c7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHotEncoding for `Wilderness Area'\n",
    "\n",
    "# INSERT YOUR CODE HERE\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "wilderness_area_ohe = ohe.fit_transform(np.array(forest_data_balanced['Wilderness_Area_Type']).reshape(-1,1))\n",
    "print('Categories for Wilderness Area: ' + str(ohe.categories_))\n",
    "\n",
    "# OneHotEncoding for `Soil Type'\n",
    "\n",
    "# INSERT YOUR CODE HERE\n",
    "ohe2 = OneHotEncoder(sparse=False)\n",
    "soil_type_ohe = ohe2.fit_transform(np.array(forest_data_balanced['Soil_Type']).reshape(-1,1))\n",
    "print('Categories for Soil Type: ' + str(ohe2.categories_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf22e54-6cf3-4c4a-8637-43cf05fba525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make pd.DataFrame with correct indices\n",
    "\n",
    "# INSERT YOUR CODE HERE\n",
    "idx = forest_data_balanced.index\n",
    "wilderness_area_ohe = pd.DataFrame(wilderness_area_ohe, columns=['Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4'], index=idx)\n",
    "soil_type_ohe = pd.DataFrame(soil_type_ohe, columns=[\n",
    "        'Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5',\n",
    "        'Soil_Type6', 'Soil_Type7', 'Soil_Type9', 'Soil_Type10',\n",
    "        'Soil_Type11', 'Soil_Type12', 'Soil_Type13', 'Soil_Type14',\n",
    "        'Soil_Type16', 'Soil_Type17', 'Soil_Type18',\n",
    "        'Soil_Type19', 'Soil_Type20', 'Soil_Type21', 'Soil_Type22',\n",
    "        'Soil_Type23', 'Soil_Type24', 'Soil_Type25', 'Soil_Type26',\n",
    "        'Soil_Type27', 'Soil_Type28', 'Soil_Type29', 'Soil_Type30',\n",
    "        'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34',\n",
    "        'Soil_Type35', 'Soil_Type36', 'Soil_Type37', 'Soil_Type38',\n",
    "        'Soil_Type39', 'Soil_Type40'], index=idx)\n",
    "\n",
    "# merge OneHot encodings with the original dataset\n",
    "forest_data_balanced = pd.concat([forest_data_balanced, wilderness_area_ohe, soil_type_ohe], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744dbdf9-7e4f-407c-a471-52ae891b30b1",
   "metadata": {},
   "source": [
    "**Task:** Now separate the dependent variable `y` from the rest of the data, `X`, and drop it. Then use the `train_test_split` function to create a test and trainings set, the test set should have size 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb0a4b1-7f81-43d8-afb4-2e27c857cc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate the target feature in an array `y` and drop it from the rest of the data `X`\n",
    "\n",
    "# INSERT YOUR CODE HERE\n",
    "y = forest_data_balanced['Cover_Type']\n",
    "X = forest_data_balanced.drop(['Cover_Type'] , axis = 1)\n",
    "labels = X.columns\n",
    "\n",
    "# test and training set split\n",
    "\n",
    "# INSERT YOUR CODE HERE\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=42)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55de7a46-0f93-4f5a-842b-b1da1e47f63f",
   "metadata": {},
   "source": [
    "In this last step, we standardise the continuous variables. That means one both centers the variable (i.e. subtracts the means) and divides by the standard deviation of the training data so that the variable has a standard deviations of 1. Importantly, the test data is standardised by using the statistics of the training data. If we standardise the whole dataset before splitting, we risk \"information leakage\" into the test set. Categorical (dummy) variables are left out and are later re-added. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08aec8f-a586-4964-a5e1-9226cff0ab27",
   "metadata": {},
   "source": [
    "**Task:** Split of the categorical and numerical parts of both trainings and test data, use the instantiated function `StandardScale` to standardise both trainings and test data. Then, rejoin the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774c3709-975a-43b8-acc4-5cda1cb2955f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split of numerical variables for standardisation\n",
    "\n",
    "# INSERT YOUR CODE HERE\n",
    "\n",
    "label = X_train.columns\n",
    "X_num_train = X_train[['Elevation', 'Aspect', 'Slope', \n",
    "                       'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology',\n",
    "                       'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon',\n",
    "                       'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points']]\n",
    "X_num_test = X_test[['Elevation', 'Aspect', 'Slope',\n",
    "                       'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology',\n",
    "                       'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon',\n",
    "                       'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points']]\n",
    "\n",
    "# standardise numerical data\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "\n",
    "# INSERT YOUR CODE HERE\n",
    "\n",
    "scaler.fit(X_num_train.values)\n",
    "X_num_train_scaled = scaler.transform(X_num_train.values)\n",
    "X_num_test_scaled = scaler.transform(X_num_test.values)\n",
    "\n",
    "# use column names and indices from above to transform scaled results in DataFrames\n",
    "\n",
    "# INSERT YOUR CODE HERE\n",
    "\n",
    "X_num_train_scaled = pd.DataFrame(X_num_train_scaled, index=X_num_train.index, columns=X_num_train.columns)\n",
    "X_num_test_scaled = pd.DataFrame(X_num_test_scaled, index=X_num_test.index, columns=X_num_test.columns)\n",
    "\n",
    "# rejoin the numerical and categorical data in two DataFrames: one for the scaled trainings and one for the scaled test data\n",
    "\n",
    "# INSERT YOUR CODE HERE\n",
    "\n",
    "train_scaled = pd.concat([X_num_train_scaled, X_train[[c for c in label if c not in X_num_train_scaled.columns.values]], y_train], axis=1)\n",
    "test_scaled = pd.concat([X_num_test_scaled, X_test[[c for c in label if c not in X_num_test_scaled.columns.values]], y_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38454c3a-63f6-456e-a0f2-e33161aa8cf3",
   "metadata": {},
   "source": [
    "**Task:** Save the two dataframes as csv files for future analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb83d1c-7452-43e4-bafd-5281d8ecd855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data as csv files\n",
    "\n",
    "# INSERT YOUR CODE HERE\n",
    "\n",
    "train_scaled.to_csv(r'Data\\covertype_train_scaled.csv', index = False)\n",
    "test_scaled.to_csv(r'Data\\covertype_test_scaled.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5729335f-6d17-42e5-8668-682cc0a5072b",
   "metadata": {},
   "source": [
    "### Sources:\n",
    "The PCA part is modelled after the course Machine Learning I by Dmitry Kobak, Uni Tübingen. \n",
    "\n",
    "Inspiration and idea for sns.PairGrid comes from Ceren Iyim (2020). [Predicting Forest Cover Types with the Machine Learning Workflow](https://towardsdatascience.com/predicting-forest-cover-types-with-the-machine-learning-workflow-1f6f049bf4df). Medium.\n",
    "\n",
    "Thanks to Philipp Holzträger and Alexander Braun for helpful comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda37552-863b-45b1-9e37-8c9493c3838e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
